\documentclass[12pt,twoside]{article}

\usepackage{jmlda}
\usepackage{datetime}
\usepackage{multicol}
\graphicspath{{./pics/}}

\pgfplotsset{compat=1.12}

\bibliographystyle{gost780u}
\graphicspath{{./pics/}}

\newcommand*{\fullref}[2]{\hyperref[{#1}]{\nameref*{#1} #2}}

\newcommand\undermat[2]{%
  \makebox[0pt][l]{$\smash{\underbrace{\phantom{%
    \begin{matrix}#2\end{matrix}}}_{\text{$#1$}}}$}#2}

\begin{document}

%\NOREVIEWERNOTES
\title
    [Классификация временных рядов]
    {Классификация временных рядов в пространстве параметров порождающих моделей}
\author
    {Карасиков~М.\,Е.}
\email
    {karasikov@phystech.edu}
\organization
    {Московский физико-технический институт}
\thanks
    {Научный руководитель: Стрижов~В.\,В.}
\abstract
    {Работа посвящена задаче многоклассовой признаковой классификации временных рядов.
    Признаковая классификация временных рядов заключается в сопоставлении каждому временному ряду его краткого признакового описания, позволяющему решать задачу классификации в пространстве признаков.
    В работе исследуются методы построения пространства признаков временных рядов.
    При этом временной ряд рассматривается как последовательность сегментов, аппроксимируемых некоторой параметрической моделью, параметры которой используются в качестве их признаковых описаний.
    Построенное так признаковое описание сегмента временного ряда наследует от модели аппроксимации такие полезные свойства, как инвариантность относительно сдвига.
    Для решения задачи классификации в качестве признаковых описаний временных рядов предлагается использовать распределения параметров аппроксимирующих сегменты моделей, что обобщает базовые методы, использующие непосредственно сами параметры аппроксимирующих моделей.
    Проведен ряд вычислительных экспериментов на реальных данных, показавших высокое качество решения задачи многоклассовой классификации.
    Эксперименты показали превосходство предлагаемого метода над базовым и многими распространенными методами классификации временных рядов на всех рассмотренных наборах данных.

		\parindent=1cm \textbf{Последние изменения}: \currenttime, \today

		\bigskip
		\textbf{Ключевые слова}: \emph {временные ряды, многоклассовая классификация, сегментация временных рядов, гиперпараметры аппроксимирующей модели, модель авторегрессии, дискретное преобразование Фурье, дискретное вейвлет"=преобразование}.}
\titleEng
    {Time series feature"~based classification}
\authorEng
    {Karasikov~M.\,E.}
\organizationEng
    {Moscow Institute of Physics and Technology}
% \abstractEng
    % {We consider time series feature"~based classification problem.
    % Classification algorithm using model parameters distribution is proposed.
    % Empirical experiments on real data are conducted.

    % \bigskip
    % \textbf{Keywords}: \emph{time"~series, feature-based classification}.}

\maketitle
%\linenumbers

\section{Введение}
\label{sec:introduction}
Временным рядом~$x$ будем называть конечную упорядоченную последовательность чисел:
\[
x = [x^{(1)}, \dots, x^{(t)}].
\]
Временные ряды являются объектом исследования в таких задачах анализа данных, как
  прогнозирование~\cite{weigend1994time, tsay2005analysis},
  обнаружение аномалий~\cite{weiss2004mining},
  сегментация~\cite{geurts2005segment},
  кластеризация~\cite{WarrenLiao20051857, zolhavarieh2014review}
  и классификация~\cite{wang2014human, Wei:2006:STS:1150402.1150498, geurts2005segment}.
Обзор по задачам и методам анализа временных рядов дается в~\cite{Esling:2012:TDM:2379776.2379788, fu2011review}.
Последние годы связаны с ростом интереса к данной области, проявляющимся в непрекращающимся предложении новых методов анализа временных рядов~---
  метрик~\cite{Ding:2008:QMT:1454159.1454226, salvador2007toward, marteau2009time},
  алгоритмов сегментации~\cite{vasko2002estimating, Esling:2012:TDM:2379776.2379788, fu2011review},
  кластеризации~\cite{frohwirth2008model, Corduas20081860, Esling:2012:TDM:2379776.2379788, fu2011review}
  и других.

В данной работе рассматривается задача классификации временных рядов, возникающая во многих приложениях
  (медицинская диагностика по ЭКГ~\cite{basil2014automatic} и ЭЭГ~\cite{marcel2007person, alomari2013automated},
  классификация типов физической активности по данным с акселерометра~\cite{6889585, Kwapisz:2011:ARU:1964897.1964918},
  верификация динамических подписей~\cite{gruber2006signature}~и~т.\,д.).

Формально задача классификации в общем виде может быть поставлена следующим образом.
Пусть~$X$~--- множество описаний объектов произвольной природы,
$Y$~--- конечное множество меток классов.
Предполагается существование целевой функции~--- отображения~$y:\;X\to Y$,
значения которого известны только на~объектах обучающей выборки
\[
    \mathfrak{D} = \left\{(x_1,y_1),\dots,(x_m,y_m)\right\} \subset X\times Y.
\]
Требуется построить алгоритм~$a:\;X\to Y$~--- отображение,
приближающее целевую функцию~$y$ на~множестве~$X$.
При $|Y|>2$ задачу классификации будем называть многоклассовой.
Задачей классификации временных рядов будем называть задачу классификации, в которой объектами классификации являются временные ряды.

Задание метрики~--- функции расстояния~\cite{Ding:2008:QMT:1454159.1454226, salvador2007toward, marteau2009time} на парах временных рядов позволяет применять метрические методы классификации.
При удачном выборе метрики дальнейшая классификация может происходить при помощи простейших метрических алгоритмов классификации, например, методом ближайшего соседа~\cite{jeong2011weighted}.
Данный подход к решению задачи классификации временных рядов чрезвычайно распространен в силу того, что позволяет свести исходную задачу классификации временных рядов к задаче выбора метрики, а также позволяет использовать graph"~based методы частичного обучения~\cite{nguyen2011positive, marussy2013success}.

Другой подход к решению задачи классификации состоит в построении для каждого временного ряда его информативного признакового описания~$\mathbf{f}:\;X\to\mathbb{R}^n$, позволяющего строить точные классификаторы с хорошей обобщающей способностью.
Построение информативного пространства признаков исходных объектов множества~$X$, позволяющего добиться заданной точности классификации и значительно упрощающего последующий анализ, является важнейшим этапом решения задачи классификации.
Признаки могут задаваться экспертом.
Так в работе~\cite{Nanopoulos01feature-basedclassification} предлагается использовать в качестве признаков статистические функции (среднее, отклонения от среднего, коэффициенты эксцесса и др.).
Стоит заметить, что при таком подходе к построению пространства признаков часто удается добиться необходимого качества классификации путем выбора соответствующих конкретной задаче признаков (см. пример ~\cite{wiens2012patient}), а сам выбор признаков становится важной технической задачей.
Второй метод построения пространства признаков заключается в задании параметрической регрессионной или аппроксимирующей модели временного ряда.
Тогда в качестве признаков временных рядов будут выступать параметры настроенной модели.
В работе~\cite{morchen2003time} в качестве признаков предлагается использовать коэффициенты дискретного преобразования Фурье (DFT), в~\cite{morchen2003time, zhang2004non}~--- дискретного вейвлет"=преобразования (DWT), а в~\cite{Corduas20081860, kini2013large, kuznetsov2015description} модели авторегрессии.
В~\cite{kalliovirta2015gaussian} исследуются свойства смеси моделей авторегрессии.
Таким образом, при данном методе построения признаковых описаний возникает задача выбора аппроксимирующей модели временного ряда. 
Об исчерпывающих исследованиях этой задачи авторам неизвестно.

В работе исследуются методы классификации временных рядов, использующие в качестве их признаковых описаний параметры аппроксимирующих моделей.
Приводится сравнение моделей аппроксимации.
Как из всякой последовательности, из временного ряда могут извлекаться его подпоследовательности, для которых может строиться признаковое описания так же, как и для исходных временных рядов.
Использование подпоследовательностей (фрагментов) позволяет обобщить алгоритмы классификации. Так в работе~\cite{geurts2005segment} предлагается алгоритм классификации временных рядов методом голосования их случайных сегментов (непрерывных подпоследовательностей со случайным начальным элементом).
В нашей работе предлагается алгоритм классификации временных рядов в пространстве распределений признаков их сегментов, который сравнивается с родственным ему алгоритмом голосования сегментов~\cite{geurts2005segment}.
В разделе~\fullref{sec:computational_experiment}{Вычислительный эксперимент} приводятся эксперименты на реальных данных, показывающие высокое качество и общность предлагаемого алгоритма в сочетании с методом признаковых описаний временных рядов параметрами аппроксимирующих их моделей.

\bigskip
\section{Постановка задачи}
\label{sec:problem_statement}
Поставим задачу многоклассовой классификации временных рядов в общем виде.
Пусть $(X,\rho)$~--- метрическое пространство временных рядов, $Y$~--- множество меток классов, $\mathfrak{D}\subset X\times Y$~--- конечная обучающая выборка.

Рассматривается семейство~$A=\left\{a:\;X\to Y\right\}$ алгоритмов классификации вида
\begin{equation}
\label{eq:classifiers}
a=b\circ \mathbf{f}\circ S,
\end{equation}
в которых
\begin{itemize}
  \item $S$~--- процедура сегментации:
  \begin{equation}
  \label{eq:segmentation}
  S:\;x\mapsto 2^{\mathbf{S}(x)},
  \end{equation}
  где $\mathbf{S}(x)$~--- множество всех сегментов временного ряда~$x$,
  \item $\mathbf{f}$~--- процедура построения признакового описания набора сегментов,
  \item $b$~--- алгоритм многоклассовой классификации.
\end{itemize}

Задана функция потерь
\[
\mathscr{L}:\;X\times Y\times Y\to \mathbb{R}
\]
и функционал качества
\[
Q(a,\mathfrak{D})=\frac{1}{|\mathfrak{D}|}\sum\limits_{(x,y)\in\mathfrak{D}}      \mathscr{L}\left(x, a(x),y\right).
\]

В качестве методов обучения~$\mu(\mathfrak{D})\in A$ будем использовать следующие:
\[
\mu_{\mathbf{f},S}(\mathfrak{D})=b^*\circ \mathbf{f}\circ S,
\]
где~$b^*$~--- минимизатор эмпирического риска:
\[
b^*=\argmin_{b}Q(b\circ \mathbf{f}\circ S,\mathfrak{D}).
\]

Оптимальный метод обучения определяется по скользящему контролю:
\[
\mu^* = \argmin_{\mu_{\mathbf{f},S}}\widehat{CV}(\mu_{\mathbf{f},S},\mathfrak{D}).
\]

\bigskip
\section{Сегментация временных рядов}
\label{sec:segmenting}
\begin{Def}
Фрагментом временного ряда~$x=[x^{(1)},\dots,x^{(t)}]$ будем называть любую его подпоследовательность~$s=[x^{(t_1)},\dots,x^{(t_k)}],$ где $1\leq t_1\leq\dots\leq t_k\leq t$.
\end{Def}
\begin{Def}
Сегментом временного ряда~$x$ будем называть его непрерывный фрагмент~$s=[x^{(i)}]_{i=t_0}^{t_1},\ 1\leq t_0\leq t_1\leq t.$
\end{Def}
\begin{Def}
Под сегментацией будем понимать функцию~\ref{eq:segmentation}, отображающую временной ряд~$x$ во множество его сегментов~$\mathbf{S}(x)$.
\end{Def}

В простейшем случае в качестве алгоритма выделения временного ряда можно взять
\begin{equation}
\label{eq:equal_fragmenting}
S:\;x\mapsto \{x\}.
\end{equation}
В таком случае будем говорить, что сегментация отсутствует.

В качестве процедуры сегментации может рассматриваться случайное выделение сегментов некоторой длины~$\ell$~\cite{geurts2005segment}.
При этом достаточно сгенерировать индексы начальных элементов, выбирая их случайно из множества~$\{1,\dots,t-\ell+1\}.$

Важным является случай квазипериодичности временного ряда, когда сам ряд состоит из похожих в определенном смысле сегментов, называемых периодами:
\begin{equation}
\label{eq:periodic}
x=\left[\underbrace{x^{(1)},\dots,x^{(t_1)}}_{s^{(1)}},\underbrace{x^{(t_1+1)},\dots,x^{(t_2)}}_{s^{(2)}},\dots,\underbrace{x^{(t_{p-1}+1)},\dots,x^{(t)}}_{s^{(p)}}\right].
\end{equation}
Тогда в качестве процедуры сегментации можно взять разбиение на периоды:
\begin{equation}
\label{eq:period_segmentation}
S:\;x\mapsto \left\{s^{(1)},\dots,s^{(p)}\right\}\in \binom{\mathbf{S}(x)}{p}.
\end{equation}
Для выделения периодов могут быть использованы, например, алгоритмы~\cite{Motrenko2015Fundamental, vasko2002estimating, Ignatov2015HumanActivity}.

\bigskip
\section{Аппроксимирующая модель сегмента временного ряда}
\label{sec:regression_model}
Поскольку сегмент временного ряда сам является временным рядом, в этом разделе слово сегмент будем опускать.

\begin{Def}
Параметрической аппроксимирующей моделью временного ряда~$x$ будем называть отображение
\begin{equation}
\label{eq:regression}
g:\;\mathbb{R}^n\times X\to X.
\end{equation}
\end{Def}
В слово <<аппроксимирующая>> вкладывается тот смысл, что модель должна приближать временной ряд:
\[
\rho(g(\mathbf{w},x), x)<\varepsilon\text{ для некоторого }\mathbf{w}\in \mathbb{R}^n.
\]
При этом естественно взять в качестве признакового описания объекта~$x$ вектор оптимальных параметров его модели.
\begin{Def}
\label{def:feature_description}
Признаковым описанием объекта~$x$, порожденным параметрической моделью~$g(\mathbf{w},x)$ назовем вектор оптимальных параметров
\begin{equation}
\label{eq:feature_solution}
\vec{f}_g(x)=
\argmin_{\mathbf{w}\in \mathbb{R}^n} \rho\left(g(\mathbf{w},x),x\right).
\end{equation}
\end{Def}

Приведем несколько примеров.
\begin{itemize}
\item \textbf{Модель линейной регрессии}.
Пусть задан многокомпонентный временной ряд~(например, время и $3$ пространственные координаты):
\[
x = [\vec{x}^{(1)}, \dots, \vec{x}^{(t)}],\text{ где }
\vec{x}^{(k)}=[x_0^{(k)},\dots,x_r^{(k)}]\T,\ k=1,\dots,t.
\]
Рассмотрим модель линейной регрессии одной из компонент временного ряда на остальные компоненты как аппроксимирующую модель:
\[
g(\mathbf{w},x)=[\hat{\vec{x}}^{(1)},\dots,\hat{\vec{x}}^{(t)}],\text{ где }
\hat{\vec{x}}^{(k)}=[\hat{x}_0^{(k)},x_1^{(k)},\dots,x_r^{(k)}]\T,\ k=1,\dots,t,
\]
\[
\hat{\vec{x}}_0=
\begin{bmatrix}
\hat{x}_0^{(1)} \\
\vdots  \\
\hat{x}_0^{(t)}
\end{bmatrix}=
\underbrace{
\begin{bmatrix}
x_1^{(1)} & \dots & x_r^{(1)} \\
\vdots         & \ddots & \vdots          \\
x_1^{(t)} & \dots & x_r^{(t)}
\end{bmatrix}
}_{X}
\underbrace{
\begin{bmatrix}
w_1 \\
\vdots  \\
w_r
\end{bmatrix}
}_{\mathbf{w}}.
\]
Тогда, выбрав в качестве~$\rho$ евклидово расстояние, по определению~\ref{def:feature_description} получим признаковое описание объекта~$x$:
\[
\vec{f}_g(x)=\left(X^{\mathsf{T}}X\right)^{-1}X^{\mathsf{T}}\hat{\vec{x}}_0.
\]

\item \textbf{Модель авторегрессии AR($p$)}.
Задан временной ряд
\[
x = [x^{(1)},\dots,x^{(t)}],\ x^{(k)}\in\mathbb{R},\ k=1,\dots,t.
\]
Выберем в качестве модели аппроксимации авторегрессионную модель порядка~$p$:
\begin{equation}
\label{eq:autoregressive_model}
g(\mathbf{w},x)=[\hat{x}^{(1)},\dots,\hat{x}^{(t)}],\text{ где }
\hat{x}^{(k)}=w_0 + \sum\limits_{i=1}^{p} w_i x^{(k-i)},\ k=1,\dots,t.
\end{equation}
Далее признаковое описание определяется аналогично случаю линейной регрессии.

\item \textbf{Дискретное преобразование Фурье}.
Задан временной ряд
\[
x = [x^{(0)},\dots,x^{(t-1)}],\ x^{(k)}\in\mathbb{R},\ k=0,\dots,t-1.
\]
Взяв в качестве аппроксимирующей модели обратное преобразование Фурье,
\[
g(\mathbf{w},x)=[\hat{x}^{(0)},\dots,\hat{x}^{(t-1)}],\text{ где }
\hat{x}^{(k)}=\frac{1}{t}\sum\limits_{j=0}^{t-1} w_j e^{\frac{2\pi i}{t}kj},\ k=0,\dots,t-1,
\]
получим, что признаковым описанием временного ряда~$x$ является прямое преобразование:
\[
\vec{f}_g(x)=[w_0,\dots,w_{t-1}],\text{ где }
w_k=\sum\limits_{j=0}^{t-1} x^{(j)} e^{-\frac{2\pi i}{t}kj},\ k=0,\dots,t-1.
\]

% \item \textbf{Дискретное вейвлет"=преобразование}.
\end{itemize}

Приведенные примеры демонстрируют большую общность построения пространства признаков при помощи модели типа~\ref{eq:regression} и решения оптимизационной задачи~\ref{eq:feature_solution}.
Вообще говоря, легко видеть, что любая функция временного ряда~$\vec{f}:X\to \mathbb{R}^n$ может быть задана как решение оптимизационной задачи~\ref{eq:feature_solution} с соответствующей моделью~\ref{eq:regression} и функцией расстояния~$\rho$.

\section{Распределения признаков сегментов}
Комбинация рассмотренных в разделах~\fullref{sec:segmenting}{Сегментация} и~\fullref{sec:regression_model}{Аппроксимирующая модель} процедур позволяет получить набор векторов параметров~--- признаковых описаний сегментов~$S(x)=\left\{s^{(1)},\dots,s^{(p)}\right\}$ временного ряда~$x$.
Имея аппроксимирующую модель~\ref{eq:regression}, получим для каждого сегмента~$s^{(k)}\in S(x)$ его признаковое описание~$\vec{f}^{(k)}:=\vec{f}_g(s^{(k)})$.

Пусть сегменты временного ряда~$x$ имеют признаковые описания, составляющие набор векторов
\begin{equation}
\label{eq:segments_features}
\left(\vec{f}^{(1)},\dots,\vec{f}^{(p)}\right).
\end{equation}
Рассмотрим набор~\ref{eq:segments_features} как реализацию случайной выборки из некоторого вероятностного распределения.
Примем следующую гипотезу, подчеркивающую то, что набор~\ref{eq:segments_features} получен для одного временного ряда~$x$.
\begin{Hypothesis}
Набор~$\left(\vec{f}^{(1)},\dots,\vec{f}^{(p)}\right)$ есть реализация простой выборки, то есть набор реализаций независимых случайных величин из общего распределения~$\mathsf{P}_0$.
\end{Hypothesis}
Распределение~$\mathsf{P}$, однако, неизвестно.
Поэтому, займемся его оценкой.
Пусть имеется параметрическое семейство распределений~$\left\{\mathsf{P}_\theta\right\}_{\theta\in \Theta}$.
Тогда, получив оценку максимального правдоподобия
\[
\hat\theta=\argmax_{\theta\in\Theta}\mathcal{L}\left(\theta\,|\,\vec{f}^{(1)},\dots,\vec{f}^{(p)}\right),
\]
будем считать её признаковым описанием исходного временного ряда.
Таким образом, задача классификации временных рядов свелась к задаче классификации параметров распределений из семейства~$\left\{\mathsf{P}_\theta\right\}_{\theta\in \Theta}$.
При этом, имея априорное распределение параметра~$F(\theta)$, целесообразно использовать вместо оценки максимального правдоподобия апостериорное математическое ожидание параметра
\[
\hat\theta=\mathsf{E}\left[\theta\,|\,\vec{f}^{(1)},\dots,\vec{f}^{(p)}\right]
=\int\limits_{\Theta}\theta\,dF\left(\theta\,|\,\vec{f}^{(1)},\dots,\vec{f}^{(p)}\right),
\]
минимизирующую средний квадрат отклонения от истинной оценки, где
\[
dF\left(\theta\,|\,\vec{f}^{(1)},\dots,\vec{f}^{(p)}\right)
=\frac{\mathcal{L}\left(\theta\,|\,\vec{f}^{(1)},\dots,\vec{f}^{(p)}\right)dF\left(\theta\right)}{\int\limits_{\Theta}\mathcal{L}\left(\theta\,|\,\vec{f}^{(1)},\dots,\vec{f}^{(p)}\right)dF\left(\theta\right)}.
\]
Заметим, что в частном случае тривиальной сегментации~\ref{eq:equal_fragmenting} и семейства вырожденных распределений оценка~$\hat\theta$ является исходным признаковым описанием.
Таким образом, предложенный подход к построению признакового описания временного ряда
\begin{equation}
\label{eq:parameter_estimation}
\mathbf{f}:\;x\mapsto\hat\theta
\end{equation}
является достаточно общим.

\bigskip
\section{Алгоритм классификации}
\label{sec:classification}
Для завершения построения алгоритма~\ref{eq:classifiers} осталось рассмотреть алгоритмы многоклассовой классификации~$b$.
К этому моменту мы уже имеем признаковое описание~$\mathbf{f}(x)$ каждого временного ряда~$x$ обучающей выборки~$\mathfrak{D}\subset X\times Y$.

\bigskip
\section{Вычислительный эксперимент}
\label{sec:computational_experiment}
Вычислительный эксперимент проводился на данных для задачи классификации типов физической активности человека.

\subsection{Датасет WISDM~\cite{Kwapisz:2011:ARU:1964897.1964918}}
содержит показания акселерометра для $6$~видов человеческой активности:
\begin{multicols}{2}
\begin{enumerate}
  \item Jogging
  \item Walking
  \item Upstairs
  \item Downstairs
  \item Sitting
  \item Standing
\end{enumerate}
\end{multicols}

Необработанные временные ряды, представляющие из себя последовательность размеченных показаний акселерометра (по тройке чисел на каждый отсчет времени с интервалом в $50$ миллисекунд), были разбиты на временные ряды длиной по $200$~отсчетов~($10$~секунд).

\begin{table}[H]
  \begin{tabular}{|c||c|c|c|c|c|c|}
    \hline
    Классы & Jogging & Walking & Upstairs & Downstairs & Sitting & Standing\\ \hline
    Число объектов & $1624$ & $2087$ & $549$ & $438$ & $276$ & $231$\\ \hline
  \end{tabular}
  \caption{Распределение временных рядов по классам. Dataset:~WISDM.}
\label{tbl:manual_usc_had}
\end{table}

\subsubsection{Ручное выделение признаков}
\paragraph{Выбор признаков.}
\label{par:manual_feature_selection}
В первом эксперименте в качестве признаковых описаний временных рядов использовались их статистические функции.
Каждой компоненте временного ряда сопоставлялись $40$~чисел~--- её среднее, стандартное отклонение, средний модуль отклонения от среднего, гистограмма с $10$ областями равной ширины.
Полученные признаки для каждой компоненты объединялись и к ним добавлялся признак средней величины ускорения.

\paragraph{Классификатор.}
Задача многоклассовой классификации сводилась к задаче бинарной классификации при помощи подхода One-vs-One.
В качестве бинарного классификатора использовался SVM с гауссовским ядром и параметрами $C=8.5,\ \gamma=0.12$.

\paragraph{Результаты.}
Для оценки качества решения использовалась процедура скользящего контроля.
Исходная обучающая выборка~$\mathfrak{D}$ случайно разбивается $m$ раз на обучающую и контрольную~($\mathfrak{D}=\mathfrak{L_i}\sqcup\mathfrak{T_i}$). В качестве внешнего критерия качества метода обучения~$\mu$ бралось
\[
\frac{1}{m}\sum_{i=1}^{m}Q(\mu(\mathfrak{L_i}),\mathfrak{T_i}),
\]
где для средней точности (accuracy) классификации объектов класса~$c\in Y$
\begin{equation}
\label{eq:class_quality}
Q_c(a,\mathfrak{T})=\frac{\sum\limits_{\substack{(x,y)\in\mathfrak{T}\\y=c}}\vec1\{a(x)=y\}}{\sum\limits_{\substack{(x,y)\in\mathfrak{T}\\y=c}}1},
\end{equation}
а для среднего качества решения задачи многоклассовой классификации
\begin{equation}
\label{eq:total_quality}
Q(a,\mathfrak{T})=\frac{1}{|\mathfrak{T}|}\sum\limits_{(x,y)\in\mathfrak{T}}\vec1\{a(x)=y\}.
\end{equation}
На диаграмме ниже приведено качество классификации, усредненное по~$m=50$ случайным разбиениям исходной выборки на тестовую и контрольную в отношении $7$ к $3$.

\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{{Accuracy_Dataset_WISDM_manual_features_nSplits_50_rate_0.7_approach_OneVsOne_HD_classifier_SVM_-t2-c8.5-g0.12}.eps}
\caption{Точность классификации при ручном выделении признаков.
Dataset:~WISDM.
Под Mean accuracy понимается значение функционала~\ref{eq:total_quality}.
Над столбцами приведены средние точности классификации для каждого класса по формуле~\ref{eq:class_quality}.}
\label{fig:WISDM_manual_features}
\end{figure}

Как видно, классы~$3$~и~$4$ хорошо отделяются от остальных.
Таким образом, если сначала отделить эту пару классов от остальных, а потом настроить бинарный классификатор для разделения классов~$3$~и~$4$, вводя дополнительные признаки (например те, которые будут рассматриваются далее), можно добиться требуемого качества решения задачи, но это не входит в цели данного эксперимента.

\subsubsection{Модель авторегрессии~\ref{eq:autoregressive_model}}
\paragraph{Признаковое описание.}
\label{par:ar_feature_selection}
Во втором эксперименте в качестве признаковых описаний временных рядов использовались все статистические функции, что брались в первом эксперименте~\ref{par:manual_feature_selection}, за исключением гистограммы.
Вместо $10$ значений для каждого блока гистограммы использовались $7$ коэффициентов модели авторегрессии AR($6$)~(см.~\ref{eq:autoregressive_model}).
Так же, проводилась предварительная нормализация признаков.

\paragraph{Классификатор.}
Задача многоклассовой классификации сводилась к задаче бинарной классификации при помощи подхода One-vs-All и экспоненциальной функцией потерь.
В качестве бинарного классификатора использовался SVM с гауссовским ядром и параметрами $C=8,\ \gamma=0.8$.

\paragraph{Результаты.}
На диаграмме ниже приведено качество классификации, усредненное по~$m=50$ случайным разбиениям исходной выборки на тестовую и контрольную в отношении $7$ к $3$.

\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{{Accuracy_Dataset_WISDM_nSplits_50_rate_0.7_approach_OneVsAll_LLB_classifier_SVM_-t2-c8-g0.8}.eps}
\caption{Точность классификации для параметров модели авторегрессии в качестве признаковых описаний.
Dataset:~WISDM.
Под Mean accuracy понимается значение функционала~\ref{eq:total_quality}.
Над столбцами приведены средние точности классификации для каждого класса по формуле~\ref{eq:class_quality}.}
\label{fig:WISDM_AR}
\end{figure}


\subsection{Датасет USC-HAD~\cite{mi12:ubicomp-sagaware}}
\label{seq:usc_had_dataset}
содержит показания акселерометра для 12 типов физической активности человека:
\begin{multicols}{2}
\begin{enumerate}
  \item walk forward
  \item walk left
  \item walk right
  \item go upstairs
  \item go downstairs
  \item run forward
  \item jump up and down
  \item sit and fidget
  \item stand
  \item sleep
  \item elevator up
  \item elevator down
\end{enumerate}
\end{multicols}

Выборка содержит примерно по $70$ шести"=компонентных временных ряда для каждого класса, а средняя длина временного ряда порядка~$3300$. Частота записи измерений сенсора~$100\,\text{Hz}$.

\subsubsection{Ручное выделение признаков}

\paragraph{Выбор признаков}
В качестве признаков брались те же признаки, что и в предыдущем эксперименте~\ref{par:manual_feature_selection}.

\paragraph{Классификатор}
Задача многоклассовой классификации сводилась к задаче бинарной классификации при помощи подхода One-vs-One.
В качестве бинарного классификатора использовался SVM с гауссовским ядром и параметрами $C=80,\ \gamma=0.002$.

\paragraph{Результаты}
Исходная выборка $100$ раз случайно разбивалась на обучающую и контрольную в отношении $7$ к $3$.
В таблице ниже приведен результат --- процент верной классификации для объектов каждого класса.

\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{{Accuracy_Dataset_USCHAD_manual_features_nSplits_100_rate_0.7_approach_OneVsOne_HD_classifier_SVM_-t2-c80-g0.002}.eps}
\caption{Точность классификации при ручном выделении признаков.
Dataset:~USC-HAD.
Под Mean accuracy понимается значение функционала~\ref{eq:total_quality}.
Над столбцами приведены средние точности классификации для каждого класса по формуле~\ref{eq:class_quality}.}
\label{fig:USCHAD_manual_features}
\end{figure}


\subsubsection{Модель авторегрессии~\ref{eq:autoregressive_model}}

\paragraph{Признаковое описание.}
\label{par:ar_feature_selection_USCHAD}
При записи данных USC-HAD сенсор делал каждую секунду $100$~измерений.
Предполагая, что на каждое <<элементарное движение>> человек тратит порядка секунды, приходим к выводу, что параметры авторегрессионной модели малых порядков в данном случае неинформативны.
Приведем исходные временные ряды к частоте $10\,\text{Hz}$ при помощи осреднения.

В качестве признаковых описаний преобразованных временных рядов возьмем статистические функции из~\ref{par:manual_feature_selection}, за исключением гистограммы.
Так же для каждой компоненты отдельно и для модуля результирующего ускорения и поворота добавим по $11$~параметров авторегрессионной модели~AR($10$)~(см.~\ref{eq:autoregressive_model}).
Затем проведем нормализация признаков.

\paragraph{Классификатор.}
Задача многоклассовой классификации сводилась к задаче бинарной классификации при помощи подхода One-vs-All и экспоненциальной функцией потерь.
В качестве бинарного классификатора использовался SVM с гауссовским ядром и параметрами $C=16,\ \gamma=0.1$.

\paragraph{Результаты.}
На диаграмме ниже приведено качество классификации, усредненное по~$m=200$ случайным разбиениям исходной выборки на тестовую и контрольную в отношении $7$ к $3$.

\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{{Accuracy_Dataset_USCHAD_nSplits_200_rate_0.7_approach_OneVsAll_LLB_classifier_SVM_-t2-c16-g0.10}.eps}
\caption{Точность классификации для параметров модели авторегрессии в качестве признаковых описаний.
Dataset:~USC-HAD.
Под Mean accuracy понимается значение функционала~\ref{eq:total_quality}.
Над столбцами приведены средние точности классификации для каждого класса по формуле~\ref{eq:class_quality}.}
\label{fig:USCHAD_AR}
\end{figure}

\subsubsection{Алгоритм голосования}
Будем решать задачу классификации только для первых $10$ классов (за исплючением <<elevator up>> и <<elevator down>>, которые плохо отделяются друг от друга и от класса <<stand>>) при помощи алгоритма голосования сегментов.

В качестве алгоритма сегментации используем разбиение исходного временного ряда на отрезки заданной длины.

Задача многоклассовой классификации решалась при помощи подхода One-vs-One с регуляризованной логистической регрессии (RLR) с параметром~$C=1$ в качестве бинарного классификатора.

На графике ниже приведены результаты для точности многоклассовой классификации, усредненные по $m=50$ случайным разбиениям.
\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{{VotingSegments_Dataset_USCHAD_nSplits_50_rate_0.7_approach_OneVsOne_HD_classifier_SVMLinear_-c1}.eps}
\caption{Зависимость точности классификации от длины сегментов для алгоритма голосования.
Dataset:~USC-HAD.
Под Mean accuracy понимается значение функционала~\ref{eq:total_quality}.}
\label{fig:USCHAD_AR_VOTING}
\end{figure}

Из графика можно видеть, что алгоритм голосования позволил повысить точность классификации на~$1\%$.

% Для сравнения проведем два эксперимента.
% В первом эксперименте аргументом модели аппроксимации будет подаваться сам ряд~\ref{eq:equal_fragmenting}, во втором эксперименте будем разбивать ряд на сегменты~\ref{eq:segment_fragmenting}.
% В качестве сегментов будем брать подотрезки временного ряда длиной~$100$, не заботясь о периодах.

\subsection{Датасет MIT-BIH Arrhythmia Database~\cite{Goldberger13062000}}

% \begin{table}[t]
  % \caption{Качество многоклассовой классификации. Data~set:~MNIST, Binary~classifier:~Vote~SVM.}
  % \begin{tabular}{|l||l|l|l|l|l|l|l|l|l|l|l|l|}
    % \hline
    % & \multicolumn{12}{c|}{Подходы} \\ \cline{2-13}
    % Сжатие $\frac{d}{n}$ & \multicolumn{3}{c|}{One-vs-All} & \multicolumn{3}{c|}{One-vs-One} & \multicolumn{3}{c|}{ECOC-Random} & \multicolumn{3}{c|}{ECOC-BCH} \\ \cline{2-13}
    % & $min$ & $avg$ & $max$ & $min$ & $avg$ & $max$ & $min$ & $avg$ & $max$ & $min$ & $avg$ & $max$\\ \hline
% 0.05 & 76.6 					& 77.3 					& 78.2 					& 82.2 					& 83.3 					& 84.4 & 75.5 & 79.6 & 81.3 & 81.2 & 82.1 & 82.7\\ \hline
  % \end{tabular}
% \label{tbl:manual_usc_had}
% \end{table}

\bigskip
\section{Заключение}
Метод классификации временных рядов в пространстве распределений параметров порождающих моделей обобщает большинство других методов классификации временных рядов и позволяет производить более тонкую настройку алгоритма классификации.

% TODO: Вычислительный эксперимент показал (покажет?), что предложенный метод дает особенно хорошее качество классификации на квазипериодических рядах~(распознавание человеческой активности, данные ЭКГ) 

\bibliography{Karasikov2015TimeSeriesClassification}
\end{document}