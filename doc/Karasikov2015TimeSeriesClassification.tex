\documentclass[12pt,twoside]{article}

\usepackage{jmlda}
\usepackage{datetime}
\usepackage{multicol}
\usepackage{rotating}

\graphicspath{{./pics/}}

\pgfplotsset{compat=1.12}

\bibliographystyle{gost780u}
\graphicspath{{./pics/}}

\newcommand*{\fullref}[1]{\hyperref[{#1}]{\autoref*{#1} (\nameref*{#1})}}

\newcommand\undermat[2]{%
  \makebox[0pt][l]{$\smash{\underbrace{\phantom{%
    \begin{matrix}#2\end{matrix}}}_{\text{$#1$}}}$}#2}

\begin{document}

%\NOREVIEWERNOTES
\title
    [Классификация временных рядов]
    {Классификация временных рядов в пространстве параметров порождающих моделей}
\author
    {Карасиков~М.\,Е.}
\email
    {karasikov@phystech.edu}
\organization
    {Московский физико-технический институт}
\thanks
    {Научный руководитель: Стрижов~В.\,В.}
\abstract
    {Работа посвящена задаче многоклассовой признаковой классификации временных рядов.
    Признаковая классификация временных рядов заключается в сопоставлении каждому временному ряду его краткого признакового описания, позволяющему решать задачу классификации в пространстве признаков.
    В работе исследуются методы построения пространства признаков временных рядов.
    При этом временной ряд рассматривается как последовательность сегментов, аппроксимируемых некоторой параметрической моделью, параметры которой используются в качестве их признаковых описаний.
    Построенное так признаковое описание сегмента временного ряда наследует от модели аппроксимации такие полезные свойства, как инвариантность относительно сдвига.
    Для решения задачи классификации в качестве признаковых описаний временных рядов предлагается использовать распределения параметров аппроксимирующих сегменты моделей, что обобщает базовые методы, использующие непосредственно сами параметры аппроксимирующих моделей.
    Проведен ряд вычислительных экспериментов на реальных данных, показавших высокое качество решения задачи многоклассовой классификации.
    Эксперименты показали превосходство предлагаемого метода над базовым и многими распространенными методами классификации временных рядов на всех рассмотренных наборах данных.

		\bigskip
		\textbf{Ключевые слова}: \emph {временные ряды, многоклассовая классификация, сегментация временных рядов, гиперпараметры аппроксимирующей модели, модель авторегрессии, дискретное преобразование Фурье, дискретное вейвлет"=преобразование}.}
\titleEng
    {Feature"~Based Time Series Classification}
\authorEng
    {Karasikov~M.\,E.}
\organizationEng
    {Moscow Institute of Physics and Technology}
% \abstractEng
    % {We consider time series feature"~based classification problem.
    % Classification algorithm using model parameters distribution is proposed.
    % Empirical experiments on real data are conducted.

    % \bigskip
    % \textbf{Keywords}: \emph{time"~series, feature-based classification}.}

%\linenumbers

\maketitle

\section{Введение}
\label{sec:introduction}
Временным рядом~$x$ будем называть конечную упорядоченную последовательность чисел:
\[
x = [x^{(1)}, \dots, x^{(t)}].
\]
Временные ряды являются объектом исследования в таких задачах анализа данных, как
  прогнозирование~\cite{weigend1994time, tsay2005analysis},
  обнаружение аномалий~\cite{weiss2004mining},
  сегментация~\cite{geurts2005segment},
  кластеризация~\cite{WarrenLiao20051857, zolhavarieh2014review}
  и классификация~\cite{wang2014human, Wei:2006:STS:1150402.1150498, geurts2005segment}.
Обзор по задачам и методам анализа временных рядов дается в~\cite{Esling:2012:TDM:2379776.2379788, fu2011review}.
Последние годы связаны с ростом интереса к данной области, проявляющимся в непрекращающимся предложении новых методов анализа временных рядов~---
  метрик~\cite{Ding:2008:QMT:1454159.1454226, salvador2007toward, marteau2009time},
  алгоритмов сегментации~\cite{vasko2002estimating, Esling:2012:TDM:2379776.2379788, fu2011review},
  кластеризации~\cite{frohwirth2008model, Corduas20081860, Esling:2012:TDM:2379776.2379788, fu2011review}
  и других.

В данной работе рассматривается задача классификации временных рядов, возникающая во многих приложениях
  (медицинская диагностика по ЭКГ~\cite{basil2014automatic} и ЭЭГ~\cite{marcel2007person, alomari2013automated},
  классификация типов физической активности по данным с акселерометра~\cite{6889585, Kwapisz:2011:ARU:1964897.1964918},
  верификация динамических подписей~\cite{gruber2006signature}~и~т.\,д.).

Формально задача классификации в общем виде может быть поставлена следующим образом.
Пусть~$X$~--- множество описаний объектов произвольной природы,
$Y$~--- конечное множество меток классов.
Предполагается существование целевой функции~--- отображения~$y:\;X\to Y$,
значения которого известны только на~объектах обучающей выборки
\[
    \mathfrak{D} = \left\{(x_1,y_1),\dots,(x_m,y_m)\right\} \subset X\times Y.
\]
Требуется построить алгоритм~$a:\;X\to Y$~--- отображение,
приближающее целевую функцию~$y$ на~множестве~$X$.
При $|Y|>2$ задачу классификации будем называть многоклассовой.
Задачей классификации временных рядов будем называть задачу классификации, в которой объектами классификации являются временные ряды.

Задание метрики~--- функции расстояния~\cite{Ding:2008:QMT:1454159.1454226, salvador2007toward, marteau2009time} на парах временных рядов позволяет применять метрические методы классификации.
При удачном выборе метрики дальнейшая классификация может происходить при помощи простейших метрических алгоритмов классификации, например, методом ближайшего соседа~\cite{jeong2011weighted}.
Данный подход к решению задачи классификации временных рядов чрезвычайно распространен в силу того, что позволяет свести исходную задачу классификации временных рядов к задаче выбора метрики, а также позволяет использовать graph"~based методы частичного обучения~\cite{nguyen2011positive, marussy2013success}.

Другой подход к решению задачи классификации состоит в построении для каждого временного ряда его информативного признакового описания~$\mathbf{f}:\;X\to\mathbb{R}^n$, позволяющего строить точные классификаторы с хорошей обобщающей способностью.
Построение информативного пространства признаков исходных объектов множества~$X$, позволяющего добиться заданной точности классификации и значительно упрощающего последующий анализ, является важнейшим этапом решения задачи классификации.
Признаки могут задаваться экспертом.
Так в работе~\cite{Nanopoulos01feature-basedclassification} предлагается использовать в качестве признаков статистические функции (среднее, отклонения от среднего, коэффициенты эксцесса и др.).
Стоит заметить, что при таком подходе к построению пространства признаков часто удается добиться необходимого качества классификации путем выбора соответствующих конкретной задаче признаков (см. пример ~\cite{wiens2012patient}), а сам выбор признаков становится важной технической задачей.
Второй метод построения пространства признаков заключается в задании параметрической регрессионной или аппроксимирующей модели временного ряда.
Тогда в качестве признаков временных рядов будут выступать параметры настроенной модели.
В работе~\cite{morchen2003time} в качестве признаков предлагается использовать коэффициенты дискретного преобразования Фурье (DFT), в~\cite{morchen2003time, zhang2004non}~--- дискретного вейвлет"=преобразования (DWT), а в~\cite{Corduas20081860, kini2013large, kuznetsov2015description} модели авторегрессии.
В~\cite{kalliovirta2015gaussian} исследуются свойства смеси моделей авторегрессии.
Таким образом, при данном методе построения признаковых описаний возникает задача выбора аппроксимирующей модели временного ряда. 
Об исчерпывающих исследованиях этой задачи авторам неизвестно.

В работе исследуются методы классификации временных рядов, использующие в качестве их признаковых описаний параметры аппроксимирующих моделей.
Приводится сравнение моделей аппроксимации.
Как из всякой последовательности, из временного ряда могут извлекаться его подпоследовательности, для которых могут строиться признаковые описания так же, как и для исходных временных рядов.
Использование подпоследовательностей (фрагментов) позволяет обобщить алгоритмы классификации. Так в работе~\cite{geurts2005segment} предлагается алгоритм классификации временных рядов методом голосования их случайных сегментов (непрерывных подпоследовательностей со случайным начальным элементом).
В нашей работе предлагается алгоритм классификации временных рядов в пространстве распределений признаков их сегментов, который сравнивается с родственным ему алгоритмом голосования сегментов~\cite{geurts2005segment}.
В~\fullref{sec:computational_experiment} приводятся эксперименты на реальных данных, показывающие высокое качество и общность предлагаемого алгоритма в сочетании с методом признаковых описаний временных рядов параметрами аппроксимирующих их моделей.

\bigskip
\section{Постановка задачи}
\label{sec:problem_statement}
Поставим задачу многоклассовой классификации временных рядов в общем виде.
Пусть $(X,\rho)$~--- метрическое пространство временных рядов, $Y$~--- множество меток классов, $\mathfrak{D}\subset X\times Y$~--- конечная обучающая выборка.

Рассматривается семейство~$A=\left\{a:\;X\to Y\right\}$ алгоритмов классификации вида
\begin{equation}
\label{eq:classifiers}
a=b\circ \mathbf{f}\circ S,
\end{equation}
в которых
\begin{itemize}
  \item $S$~--- процедура сегментации:
  \begin{equation}
  \label{eq:segmentation}
  S:\;x\mapsto 2^{\mathbf{S}(x)},
  \end{equation}
  где $\mathbf{S}(x)$~--- множество всех сегментов временного ряда~$x$,
  \item $\mathbf{f}$~--- процедура построения признакового описания набора сегментов,
  \item $b$~--- алгоритм многоклассовой классификации.
\end{itemize}

Задана функция потерь
\[
\mathscr{L}:\;X\times Y\times Y\to \mathbb{R}
\]
и функционал качества
\begin{equation}
\label{eq:empirical_risk}
Q(a,\mathfrak{D})=\frac{1}{|\mathfrak{D}|}\sum\limits_{(x,y)\in\mathfrak{D}}      \mathscr{L}\left(x, a(x),y\right).
\end{equation}

В качестве методов обучения~$\mu(\mathfrak{D})\in A$ будем использовать следующие:
\[
\mu_{\mathbf{f},S}(\mathfrak{D})=b^*\circ \mathbf{f}\circ S,
\]
где~$b^*$~--- минимизатор эмпирического риска:
\[
b^*=\argmin_{b}Q(b\circ \mathbf{f}\circ S,\mathfrak{D}).
\]

Оптимальный метод обучения определяется по скользящему контролю:
\[
\mu^* = \argmin_{\mu_{\mathbf{f},S}}\widehat{CV}(\mu_{\mathbf{f},S},\mathfrak{D}).
\]

\bigskip
\section{Сегментация временных рядов}
\label{sec:segmenting}
\begin{Def}
Фрагментом временного ряда~$x=[x^{(1)},\dots,x^{(t)}]$ будем называть любую его подпоследовательность~$s=[x^{(t_1)},\dots,x^{(t_k)}],$ где $1\leq t_1<\dotso< t_k\leq t$.
\end{Def}
\begin{Def}
Сегментом временного ряда~$x$ будем называть его непрерывный фрагмент~$s=[x^{(i)}]_{i=t_0}^{t_1},\ 1\leq t_0\leq t_1\leq t.$
\end{Def}
\begin{Def}
Под сегментацией будем понимать отображение временных рядов во множество их сегментов~(\autoref{eq:segmentation}).
\end{Def}

Примеры.
\begin{itemize}
\item
  Тривиальная сегментация
  \begin{equation}
  \label{eq:equal_fragmenting}
  S:\;x\mapsto \{x\}.
  \end{equation}

\item
  Случайное выделение сегментов некоторой длины~$\ell$~\cite{geurts2005segment}.

\item
  Важным является случай квазипериодичности временного ряда, когда сам ряд состоит из похожих в определенном смысле сегментов, называемых периодами:
  \begin{equation}
  \label{eq:periodic}
  x=\left[\underbrace{x^{(1)},\dots,x^{(t_1)}}_{s^{(1)}},\underbrace{x^{(t_1+1)},\dots,x^{(t_2)}}_{s^{(2)}},\dots,\underbrace{x^{(t_{p-1}+1)},\dots,x^{(t)}}_{s^{(p)}}\right].
  \end{equation}
  Тогда в качестве процедуры сегментации можно взять разбиение на периоды:
  \begin{equation}
  \label{eq:period_segmentation}
  S:\;x\mapsto \left\{s^{(1)},\dots,s^{(p)}\right\}.
  \end{equation}
  Для выделения периодов могут быть использованы, например, алгоритмы~\cite{Motrenko2015Fundamental, vasko2002estimating, Ignatov2015HumanActivity}.
\end{itemize}

\bigskip
\section{Аппроксимирующая модель сегмента временного ряда}
\label{sec:regression_model}
Поскольку сегмент временного ряда сам является временным рядом, в этом разделе слово сегмент будем опускать.

\begin{Def}
Параметрической аппроксимирующей моделью временного ряда~$x$ будем называть отображение
\begin{equation}
\label{eq:regression}
g:\;\mathbb{R}^n\times X\to X.
\end{equation}
\end{Def}
В слово <<аппроксимирующая>> вкладывается тот смысл, что модель должна приближать временной ряд:
\[
\rho(g(\mathbf{w},x), x)<\varepsilon\text{ для некоторого }\mathbf{w}\in \mathbb{R}^n.
\]
При этом естественно взять в качестве признакового описания объекта~$x$ вектор оптимальных параметров его модели.
\begin{Def}
\label{def:feature_description}
Признаковым описанием объекта~$x$, порожденным параметрической моделью~$g(\mathbf{w},x)$ назовем вектор оптимальных параметров этой модели
\begin{equation}
\label{eq:feature_solution}
\vec{f}_g(x)=
\argmin_{\mathbf{w}\in \mathbb{R}^n} \rho\left(g(\mathbf{w},x),x\right).
\end{equation}
\end{Def}

Приведем несколько примеров.
\begin{itemize}
\item \textbf{Модель линейной регрессии}.
Пусть задан многокомпонентный временной ряд~(например, время и $3$ пространственные координаты):
\[
x = [\vec{x}^{(1)}, \dots, \vec{x}^{(t)}],\text{ где }
\vec{x}^{(k)}=[x_0^{(k)},\dots,x_r^{(k)}]\T,\ k=1,\dots,t.
\]
Рассмотрим модель линейной регрессии одной из компонент временного ряда на остальные компоненты как аппроксимирующую модель:
\[
g(\mathbf{w},x)=[\hat{\vec{x}}^{(1)},\dots,\hat{\vec{x}}^{(t)}],\text{ где }
\hat{\vec{x}}^{(k)}=[\hat{x}_0^{(k)},x_1^{(k)},\dots,x_r^{(k)}]\T,\ k=1,\dots,t,
\]
\[
\hat{\vec{x}}_0=
\begin{bmatrix}
\hat{x}_0^{(1)} \\
\vdots  \\
\hat{x}_0^{(t)}
\end{bmatrix}=
\underbrace{
\begin{bmatrix}
x_1^{(1)} & \dots & x_r^{(1)} \\
\vdots         & \ddots & \vdots          \\
x_1^{(t)} & \dots & x_r^{(t)}
\end{bmatrix}
}_{X}
\underbrace{
\begin{bmatrix}
w_1 \\
\vdots  \\
w_r
\end{bmatrix}
}_{\mathbf{w}}.
\]
Тогда, выбрав в качестве~$\rho$ евклидово расстояние, по определению~\ref{def:feature_description} получим признаковое описание объекта~$x$:
\[
\vec{f}_g(x)=\left(X^{\mathsf{T}}X\right)^{-1}X^{\mathsf{T}}\hat{\vec{x}}_0.
\]

\item \textbf{Модель авторегрессии AR($p$)}.
Задан временной ряд
\[
x = [x^{(1)},\dots,x^{(t)}],\ x^{(k)}\in\mathbb{R},\ k=1,\dots,t.
\]
Выберем в качестве модели аппроксимации авторегрессионную модель порядка~$p$:
\begin{equation}
\label{eq:autoregressive_model}
g(\mathbf{w},x)=[\hat{x}^{(1)},\dots,\hat{x}^{(t)}],\text{ где }
\hat{x}^{(k)}=w_0 + \sum\limits_{i=1}^{p} w_i x^{(k-i)},\ k=1,\dots,t.
\end{equation}
Далее признаковое описание определяется аналогично случаю линейной регрессии.

\item \textbf{Дискретное преобразование Фурье}.
Задан временной ряд
\[
x = [x^{(0)},\dots,x^{(t-1)}],\ x^{(k)}\in\mathbb{R},\ k=0,\dots,t-1.
\]
Взяв в качестве аппроксимирующей модели обратное преобразование Фурье,
\[
g(\mathbf{w},x)=[\hat{x}^{(0)},\dots,\hat{x}^{(t-1)}],\text{ где }
\hat{x}^{(k)}=\frac{1}{t}\sum\limits_{j=0}^{t-1} w_j e^{\frac{2\pi i}{t}kj},\ k=0,\dots,t-1,
\]
получим, что признаковым описанием временного ряда~$x$ является прямое преобразование:
\begin{equation}
\label{eq:fourier}
\vec{f}_g(x)=\left[\Re w_0,\Im w_0,\dots,\Re w_{t-1},\Im w_{t-1}\right],\text{ где }
w_k=\sum\limits_{j=0}^{t-1} x^{(j)} e^{-\frac{2\pi i}{t}kj},\ k=0,\dots,t-1.
\end{equation}

% \item \textbf{Дискретное вейвлет"=преобразование}.
\end{itemize}

Приведенные примеры демонстрируют большую общность построения пространства признаков при помощи моделей типа~\ref{eq:regression} и решения оптимизационной задачи~\ref{eq:feature_solution}.
Вообще говоря, легко видеть, что при $|X|\geq 2$ любая процедура построения признаковых описаний~$\vec{f}:\;X\to \mathbb{R}^n$ задается решением оптимизационной задачи~\ref{eq:feature_solution} при выборе соответствующей пары~$(g,\rho)$.

\section{Распределения признаков сегментов}
\label{sec:distribution}
Объединим идеи, изложенные в~\fullref{sec:segmenting} и~\fullref{sec:regression_model}.
Согласно аппроксимирующей модели~\ref{eq:regression} получим для каждого сегмента~$s^{(k)}\in S(x)=\left\{s^{(1)},\dots,s^{(p)}\right\}$ временного ряда~$x$ его признаковое описание~$\vec{f}^{(k)}:=\vec{f}_g(s^{(k)})$, решив оптимизационную задачу~\ref{eq:feature_solution}.
Тогда всему набору сегментов~$S(x)$ будет соответствовать выборка
\begin{equation}
\label{eq:segments_features}
\left(\vec{f}^{(1)},\dots,\vec{f}^{(p)}\right).
\end{equation}
Будем рассматривать вероятностную модель.
\begin{Hypothesis}
Выборка~$\left(\vec{f}^{(1)},\dots,\vec{f}^{(p)}\right)$~--- простая, то есть случайная, независимая и однородная, где $\vec{f}^{(k)}\sim\mathsf{P}_0$.
\end{Hypothesis}
Далее будем рассматривать вопрос оценки распределения~$\mathsf{P}_0$.

Пусть имеется параметрическое семейство распределений~$\left\{\mathsf{P}_{\vec\theta}\right\}_{\vec\theta\in \Theta}$.
Тогда, получив оценку максимального правдоподобия
\[
\hat{\vec\theta}=\argmax_{\vec\theta\in\Theta}\mathcal{L}\left(\vec\theta\,|\,\vec{f}^{(1)},\dots,\vec{f}^{(p)}\right),
\]
будем считать её признаковым описанием исходного временного ряда.
Таким образом, задача классификации временных рядов свелась к задаче классификации параметров распределений из семейства~$\left\{\mathsf{P}_{\vec\theta}\right\}_{\vec\theta\in \Theta}$.
При этом, имея априорное распределение параметра~$F(\vec\theta)$, целесообразно использовать вместо оценки максимального правдоподобия апостериорное математическое ожидание параметра
\[
\hat{\vec\theta}=\mathsf{E}\left[\vec\theta\,|\,\vec{f}^{(1)},\dots,\vec{f}^{(p)}\right]
=\int\limits_{\Theta}\vec\theta\,dF\left(\vec\theta\,|\,\vec{f}^{(1)},\dots,\vec{f}^{(p)}\right),
\]
минимизирующее средний квадрат отклонения от истинного значения параметра, где
\[
dF\left(\vec\theta\,|\,\vec{f}^{(1)},\dots,\vec{f}^{(p)}\right)
=\frac{\mathcal{L}\left(\vec\theta\,|\,\vec{f}^{(1)},\dots,\vec{f}^{(p)}\right)dF\left(\vec\theta\right)}{\int\limits_{\Theta}\mathcal{L}\left(\vec\theta\,|\,\vec{f}^{(1)},\dots,\vec{f}^{(p)}\right)dF\left(\vec\theta\right)}.
\]
Заметим, что в частном случае тривиальной сегментации~\ref{eq:equal_fragmenting} и семейства вырожденных распределений оценка~$\hat{\vec\theta}$ является исходным признаковым описанием.
Таким образом, предложенный подход к построению признакового описания временного ряда
\begin{equation}
\label{eq:parameter_estimation}
\mathbf{f}:\;x\mapsto\hat{\vec\theta}
\end{equation}
является достаточно общим и при этом хорошо интерпретируется.

\bigskip
\section{Алгоритм классификации}
\label{sec:classification}
Для завершения построения алгоритма~\ref{eq:classifiers} осталось рассмотреть алгоритмы многоклассовой классификации~$b$.
К этому моменту мы уже имеем признаковое описание~$\mathbf{f}(x)$ для каждого временного ряда~$x$ обучающей выборки~$\mathfrak{D}\subset X\times Y$.

Решать задачу многоклассовой классификации будем сведением её к задачам бинарной классификации.
Наиболее общей стратегией к сведению является Error"=Correcting Output Codes~\cite{ass-rmbuamc-00}, которая обобщает стратегии One-vs-All и One-vs-One.

В~нашей работе для решения задач бинарной классификации, где~$Y=\{-1,+1\}$, берутся регуляризованная логистическая регрессия (RLR) и различные модификации SVM.

\begin{itemize}
\item Классификатор \textbf{SVM}
% \cite{Cortes95support-vectornetworks}
выглядит следующим образом:
\[
f(x;\vec w,w_0)=\sign\left(\vec{w}^{\mathsf{T}}\mathbf{f}(x)-w_0\right),
\]
где параметры $\vec{w}$ и $w_0$ определяются решением задачи безусловной минимизации
\[
\frac{1}{2C} \|\vec w\|^2_2 + \sum_{(x,y)\in\mathfrak{D}}\max\left\{1-y(\vec{w}^\mathsf{T}\mathbf{f}(x) - w_0),\,0\right\} \to \min_{\vec w\in\mathbb{R}^n,\;w_0\in\mathbb{R}}.
\]
\item Линейный классификатор \textbf{RLR}
% \cite{jaakkola2000bayesian}
записывается в виде
\[
f(x;\vec w)=\sign\vec{w}^{\mathsf{T}}\mathbf{f}(x),
\]
где вектор параметров $\vec{w}$ определяется из условия
\[
\frac{1}{2C} \|\vec w\|^2_2 + \sum_{(x,y)\in\mathfrak{D}}\log\left(1 + e^{-y\vec{w}^\mathsf{T}\mathbf{f}(x)}\right)\to \min_{\vec w\in\mathbb{R}^n}.
\]
\end{itemize}

\bigskip
\section{Вычислительный эксперимент}
\label{sec:computational_experiment}
Вычислительный эксперимент проводился на данных для задачи классификации типов физической активности человека.

\subsection{Датасет WISDM}
Датасет WISDM~\cite{Kwapisz:2011:ARU:1964897.1964918} содержит показания акселерометра для $6$~видов человеческой активности:
\begin{multicols}{2}
\begin{enumerate}
  \item Jogging
  \item Walking
  \item Upstairs
  \item Downstairs
  \item Sitting
  \item Standing
\end{enumerate}
\end{multicols}

Необработанные данные, представляющие из себя последовательность размеченных показаний акселерометра (по тройке чисел на каждый отсчет времени с интервалом в $50$ миллисекунд), были разбиты на временные ряды длиной по $200$~отсчетов~($10$~секунд).

\begin{table}[H]
  \begin{tabular}{|c||c|c|c|c|c|c|}
    \hline
    Классы & Jogging & Walking & Upstairs & Downstairs & Sitting & Standing\\ \hline
    Число объектов & $1624$ & $2087$ & $549$ & $438$ & $276$ & $231$\\ \hline
  \end{tabular}
  \caption{Распределение временных рядов по классам. Dataset:~WISDM.}
\label{tbl:manual_usc_had}
\end{table}

\subsubsection{Ручное выделение признаков}
\paragraph{Выбор признаков.}
\label{par:manual_feature_selection}
В первом эксперименте в качестве признаковых описаний временных рядов использовались их статистические функции.
Каждой компоненте временного ряда сопоставлялись $40$~чисел~--- её среднее, стандартное отклонение, средний модуль отклонения от среднего, гистограмма с $10$ областями равной ширины.
Полученные признаки для каждой компоненты объединялись и к ним добавлялся признак средней величины ускорения.

\paragraph{Классификатор.}
Задача многоклассовой классификации сводилась к задаче бинарной классификации при помощи подхода One-vs-One.
В качестве бинарного классификатора использовался SVM с гауссовским ядром и параметрами $C=8.5,\ \gamma=0.12$.

\paragraph{Результаты.}
Для оценки качества решения использовалась процедура скользящего контроля.
Исходная обучающая выборка~$\mathfrak{D}$ случайно разбивается $m$ раз на обучающую и контрольную~($\mathfrak{D}=\mathfrak{L_i}\sqcup\mathfrak{T_i}$). В качестве внешнего критерия качества метода обучения~$\mu$ бралось
\[
\frac{1}{m}\sum_{i=1}^{m}Q(\mu(\mathfrak{L_i}),\mathfrak{T_i}),
\]
где для средней точности (accuracy) классификации объектов класса~$c\in Y$
\begin{equation}
\label{eq:class_quality}
Q_c(a,\mathfrak{T})=\frac{\sum\limits_{\substack{(x,y)\in\mathfrak{T}\\y=c}}\vec1\{a(x)=y\}}{\sum\limits_{\substack{(x,y)\in\mathfrak{T}\\y=c}}1},
\end{equation}
а для среднего качества решения задачи многоклассовой классификации
\begin{equation}
\label{eq:total_quality}
Q(a,\mathfrak{T})=\frac{1}{|\mathfrak{T}|}\sum\limits_{(x,y)\in\mathfrak{T}}\vec1\{a(x)=y\}.
\end{equation}
На диаграмме ниже (см.~\ref{fig:WISDM_manual_features}) приведено качество классификации, усредненное по~$m=50$ случайным разбиениям исходной выборки на тестовую и контрольную в пропорциях $7$~к~$3$.

\begin{minipage}{\textwidth}
\begin{minipage}{0.49\textwidth}
\begin{figure}[H]
\includegraphics[width=\textwidth]{{Accuracy_Dataset_WISDM_manual_features_nSplits_50_rate_0.7_approach_OneVsOne_HD_classifier_SVM_-t2-c8.5-g0.12}.eps}
\caption{Точность классификации при ручном выделении признаков.
Dataset:~WISDM.
Под Mean accuracy понимается значение функционала~\ref{eq:total_quality}.
Над столбцами приведены средние точности классификации для каждого класса по формуле~\ref{eq:class_quality}.}
\label{fig:WISDM_manual_features}
\end{figure}
\end{minipage}
\quad
\begin{minipage}{0.49\textwidth}
\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|c|}
  \cline{3-8}
  \multicolumn{2}{c|}{} & \multicolumn{6}{c|}{Predicted class} \\ \cline{3-8}
  \multicolumn{2}{c|}{} & $1$ & $2$ & $3$ & $4$ & $5$ & $6$\\ \cline{1-8}
  \multirow{6}{*}{\begin{sideways}Actual class\end{sideways}}
  & $1$ & $\vec{1.00}$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$\\ \cline{2-8}
  & $2$ & $0.00$ & $\vec{0.99}$ & $0.01$ & $0.00$ & $0.00$ & $0.00$\\ \cline{2-8}
  & $3$ & $0.03$ & $0.04$ & $\vec{0.90}$ & $0.04$ & $0.00$ & $0.00$\\ \cline{2-8}
  & $4$ & $0.01$ & $0.05$ & $0.05$ & $\vec{0.89}$ & $0.00$ & $0.00$\\ \cline{2-8}
  & $5$ & $0.01$ & $0.00$ & $0.00$ & $0.00$ & $\vec{0.98}$ & $0.00$\\ \cline{2-8}
  & $6$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $\vec{1.00}$\\ \cline{1-8}
\end{tabular}}
\caption{Mean confusion matrix. Ручное выделение признаков. Dataset:~WISDM.}
\label{tbl:WISDM_manual_confusion}
\end{table}
\end{minipage}
\end{minipage}

Как видно из таблицы~\ref{tbl:WISDM_manual_confusion}, классы~$2$,~$3$~и~$4$ не достаточно хорошо отделяются друг от друга.

\subsubsection{Модель авторегрессии~\ref{eq:autoregressive_model}}
\paragraph{Признаковое описание.}
\label{par:ar_feature_selection}
Во втором эксперименте в качестве признаковых описаний временных рядов использовались все статистические функции, что брались в первом эксперименте,~\fullref{par:manual_feature_selection}, за исключением гистограммы.
Вместо $10$ значений для каждого блока гистограммы использовались $7$ коэффициентов модели авторегрессии AR($6$)~(см.~\ref{eq:autoregressive_model}).
Таким образом, каждый временной ряд описывался~$31$ числами.
Так же, проводилась предварительная нормализация признаков.

\paragraph{Классификатор.}
Задача многоклассовой классификации сводилась к задаче бинарной классификации при помощи подхода One-vs-All и линейной функцией потерь.
В качестве бинарного классификатора использовался SVM с гауссовским ядром и параметрами $C=8,\ \gamma=0.8$.

\paragraph{Результаты.}
На диаграмме ниже приведено качество классификации, усредненное по~$m=50$ случайным разбиениям исходной выборки на тестовую и контрольную в отношении $7$ к $3$.

\begin{minipage}{\textwidth}
\begin{minipage}{0.49\textwidth}
\begin{figure}[H]
\includegraphics[width=\textwidth]{{Accuracy_Dataset_WISDM_nSplits_50_rate_0.7_approach_OneVsAll_LLB_classifier_SVM_-t2-c8-g0.8}.eps}
\caption{Точность классификации для параметров модели авторегрессии в качестве признаковых описаний.
Dataset:~WISDM.
Под Mean accuracy понимается значение функционала~\ref{eq:total_quality}.
Над столбцами приведены средние точности классификации для каждого класса по формуле~\ref{eq:class_quality}.}
\label{fig:WISDM_AR}
\end{figure}
\end{minipage}
\quad
\begin{minipage}{0.49\textwidth}
\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|c|}
  \cline{3-8}
  \multicolumn{2}{c|}{} & \multicolumn{6}{c|}{Predicted class} \\ \cline{3-8}
  \multicolumn{2}{c|}{} & $1$ & $2$ & $3$ & $4$ & $5$ & $6$\\ \cline{1-8}
  \multirow{6}{*}{\begin{sideways}Actual class\end{sideways}}
  & $1$ & $\vec{1.00}$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$\\ \cline{2-8}
  & $2$ & $0.00$ & $\vec{0.99}$ & $0.00$ & $0.00$ & $0.00$ & $0.00$\\ \cline{2-8}
  & $3$ & $0.01$ & $0.01$ & $\vec{0.96}$ & $0.02$ & $0.00$ & $0.00$\\ \cline{2-8}
  & $4$ & $0.00$ & $0.02$ & $0.04$ & $\vec{0.94}$ & $0.00$ & $0.00$\\ \cline{2-8}
  & $5$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $\vec{0.98}$ & $0.01$\\ \cline{2-8}
  & $6$ & $0.01$ & $0.00$ & $0.00$ & $0.00$ & $0.01$ & $\vec{0.98}$\\ \cline{1-8}
\end{tabular}}
\caption{Mean confusion matrix. Признаки, порожденные моделью авторегрессии. Dataset:~WISDM.}
\label{tbl:WISDM_AR_confusion}
\end{table}
\end{minipage}
\end{minipage}
Таким образом, несмотря на неравномерное распределение объектов по классам, использование признакового описания, порожденного моделью авторегрессии, позволяет значительно повысить качество классификации.
Точность построенного классификатора минимальна для~$4$ класса <<Downstairs>> и составляет~$94.2\%$.
Алгоритм классификации можно улучшать добавлением дополнительных признаков, например, параметров линейной регрессии, однако, добиваться максимально возможной точности не входит в цели нашего эксперимента.

\subsection{Датасет USC-HAD}
\label{seq:usc_had_dataset}
Датасет USC-HAD~\cite{mi12:ubicomp-sagaware} содержит показания акселерометра для $12$~типов физической активности человека:
\begin{multicols}{2}
\begin{enumerate}
  \item walk forward
  \item walk left
  \item walk right
  \item go upstairs
  \item go downstairs
  \item run forward
  \item jump up and down
  \item sit and fidget
  \item stand
  \item sleep
  \item elevator up
  \item elevator down
\end{enumerate}
\end{multicols}

Выборка содержит примерно по $70$ шести"=компонентных временных ряда для каждого класса, а средняя длина временного ряда порядка~$3300$. Частота записи измерений сенсора~$100\,\text{Hz}$.

\subsubsection{Ручное выделение признаков}

\paragraph{Выбор признаков}
В качестве признаков брались те же признаки, что и в предыдущем эксперименте,~\fullref{par:manual_feature_selection}.

\paragraph{Классификатор}
Задача многоклассовой классификации сводилась к задаче бинарной классификации при помощи подхода One-vs-One.
В качестве бинарного классификатора использовался SVM с гауссовским ядром и параметрами $C=80,\ \gamma=0.002$.

\paragraph{Результаты}
Исходная выборка $100$ раз случайно разбивалась на обучающую и контрольную в отношении $7$ к $3$.
На диаграмме~\ref{fig:USCHAD_manual_features} приведен результат~--- процент верной классификации для объектов каждого класса.

\begin{minipage}{\textwidth}
\begin{minipage}{0.49\textwidth}
\begin{figure}[H]
\includegraphics[width=\textwidth]{{Accuracy_Dataset_USCHAD_manual_features_nSplits_100_rate_0.7_approach_OneVsOne_HD_classifier_SVM_-t2-c80-g0.002}.eps}
\caption{Точность классификации при ручном выделении признаков.
Dataset:~USC-HAD.
Под Mean accuracy понимается значение функционала~\ref{eq:total_quality}.
Над столбцами приведены средние точности классификации для каждого класса по формуле~\ref{eq:class_quality}.}
\label{fig:USCHAD_manual_features}
\end{figure}
\end{minipage}
\quad
\begin{minipage}{0.49\textwidth}
\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
  \cline{3-14}
  \multicolumn{2}{c|}{} & \multicolumn{12}{c|}{Predicted class} \\ \cline{3-14}
  \multicolumn{2}{c|}{} & $1$  & $2$  & $3$  & $4$  & $5$  & $6$  & $7$  & $8$  & $9$ & $10$ & $11$ & $12$\\ \cline{1-14}
  \multirow{12}{*}{\begin{sideways}Actual class\end{sideways}}
  & $1$ & $\vec{0.89}$ & $0.00$ & $0.00$ & $0.04$ & $0.03$ & $0.01$ & $0.03$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$\\ \cline{2-14}
  & $2$ & $0.00$ & $\vec{0.93}$ & $0.01$ & $0.00$ & $0.04$ & $0.01$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$\\ \cline{2-14}
  & $3$ & $0.00$ & $0.01$ & $\vec{0.88}$ & $0.07$ & $0.00$ & $0.02$ & $0.03$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$\\ \cline{2-14}
  & $4$ & $0.02$ & $0.01$ & $0.10$ & $\vec{0.74}$ & $0.08$ & $0.01$ & $0.03$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$\\ \cline{2-14}
  & $5$ & $0.05$ & $0.06$ & $0.00$ & $0.04$ & $\vec{0.78}$ & $0.01$ & $0.06$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$\\ \cline{2-14}
  & $6$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $\vec{0.98}$ & $0.01$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$\\ \cline{2-14}
  & $7$ & $0.01$ & $0.00$ & $0.00$ & $0.01$ & $0.05$ & $0.09$ & $\vec{0.84}$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$\\ \cline{2-14}
  & $8$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $\vec{0.93}$ & $0.02$ & $0.01$ & $0.02$ & $0.03$\\ \cline{2-14}
  & $9$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.02$ & $\vec{0.86}$ & $0.00$ & $0.07$ & $0.05$\\ \cline{2-14}
  & $10$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.02$ & $0.00$ & $\vec{0.98}$ & $0.00$ & $0.00$\\ \cline{2-14}
  & $11$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.01$ & $0.03$ & $0.00$ & $\vec{0.54}$ & $0.43$\\ \cline{2-14}
  & $12$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.01$ & $0.04$ & $0.00$ & $0.40$ & $\vec{0.55}$\\ \cline{1-14}
\end{tabular}}
\caption{Mean confusion matrix. Ручное выделение признаков. Dataset:~USC-HAD.}
\label{tbl:USCHAD_manual_confusion}
\end{table}
\end{minipage}
\end{minipage}

Из таблицы~\ref{tbl:USCHAD_manual_confusion} видно, что классы~$11$~и~$12$ (elevator up и elevator down) очень плохо отделяются друг от друга, то есть статистические признаки не достаточно чувствительны, чтобы разделить эти классы.
Также ошибка на классах~$4$~и~$5$ (go upstairs и go downstairs) превышает~$20\%$.

\subsubsection{Модель авторегрессии~\ref{eq:autoregressive_model}}

\paragraph{Признаковое описание.}
\label{par:ar_feature_selection_USCHAD}
При записи данных USC-HAD сенсор делал каждую секунду $100$~измерений.
Предполагая, что на каждое <<элементарное движение>> человек тратит порядка секунды, приходим к выводу, что параметры авторегрессионной модели малых порядков в данном случае неинформативны.
Приведем исходные временные ряды к частоте $10\,\text{Hz}$ при помощи осреднения.

В качестве признаковых описаний преобразованных временных рядов возьмем статистические функции, описанные в~\fullref{par:manual_feature_selection}, за исключением гистограммы.
Так же для каждой компоненты отдельно и для модуля результирующего ускорения и поворота добавим по $11$~параметров авторегрессионной модели~AR($10$)~(см.~\autoref{eq:autoregressive_model}).
Затем проведем нормализация признаков.

\paragraph{Классификатор.}
Задача многоклассовой классификации сводилась к задаче бинарной классификации при помощи подхода One-vs-All и линейной функцией потерь.
В качестве бинарного классификатора использовался SVM с гауссовским ядром и параметрами $C=16,\ \gamma=0.1$.

\paragraph{Результаты.}
На диаграмме ниже приведено качество классификации, усредненное по~$m=200$ случайным разбиениям исходной выборки на тестовую и контрольную в отношении $7$ к $3$.

\begin{minipage}{\textwidth}
\begin{minipage}{0.49\textwidth}
\begin{figure}[H]
\includegraphics[width=\textwidth]{{Accuracy_Dataset_USCHAD_nSplits_200_rate_0.7_approach_OneVsAll_LLB_classifier_SVM_-t2-c16-g0.10}.eps}
\caption{Точность классификации для параметров модели авторегрессии в качестве признаковых описаний.
Dataset:~USC-HAD.
Под Mean accuracy понимается значение функционала~\ref{eq:total_quality}.
Над столбцами приведены средние точности классификации для каждого класса по формуле~\ref{eq:class_quality}.}
\label{fig:USCHAD_AR}
\end{figure}
\end{minipage}
\quad
\begin{minipage}{0.49\textwidth}
\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
  \cline{3-14}
  \multicolumn{2}{c|}{} & \multicolumn{12}{c|}{Predicted class} \\ \cline{3-14}
  \multicolumn{2}{c|}{} & $1$  & $2$  & $3$  & $4$  & $5$  & $6$  & $7$  & $8$  & $9$ & $10$ & $11$ & $12$\\ \cline{1-14}
  \multirow{12}{*}{\begin{sideways}Actual class\end{sideways}}
  & $1$ & $\vec{0.93}$ & $0.01$ & $0.03$ & $0.02$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$\\ \cline{2-14}
  & $2$ & $0.01$ & $\vec{0.98}$ & $0.01$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$\\ \cline{2-14}
  & $3$ & $0.02$ & $0.02$ & $\vec{0.97}$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$\\ \cline{2-14}
  & $4$ & $0.00$ & $0.00$ & $0.00$ & $\vec{0.98}$ & $0.01$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$\\ \cline{2-14}
  & $5$ & $0.03$ & $0.00$ & $0.00$ & $0.01$ & $\vec{0.94}$ & $0.02$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$\\ \cline{2-14}
  & $6$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $\vec{1.00}$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$\\ \cline{2-14}
  & $7$ & $0.00$ & $0.00$ & $0.00$ & $0.01$ & $0.00$ & $0.00$ & $\vec{0.99}$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$\\ \cline{2-14}
  & $8$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $\vec{0.92}$ & $0.06$ & $0.00$ & $0.02$ & $0.00$\\ \cline{2-14}
  & $9$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.02$ & $\vec{0.95}$ & $0.00$ & $0.02$ & $0.00$\\ \cline{2-14}
  & $10$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $\vec{1.00}$ & $0.00$ & $0.00$\\ \cline{2-14}
  & $11$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $\vec{0.75}$ & $0.25$\\ \cline{2-14}
  & $12$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.01$ & $0.01$ & $0.00$ & $0.22$ & $\vec{0.75}$\\ \cline{1-14}
\end{tabular}}
\caption{Mean confusion matrix. Признаки, порожденные моделью авторегрессии. Dataset:~USC-HAD.}
\label{tbl:USCHAD_AR_confusion}
\end{table}
\end{minipage}
\end{minipage}

Из таблицы~\ref{tbl:USCHAD_AR_confusion} видно, что использование признакового описания, порожденного моделью авторегрессии, значительно повысило качество классификации для всех классов.
Недостаточно точно отделяются только классы~$11$~и~$12$ (elevator up и elevator down), где ошибка составляет~$25\%$.
Однако, они превосходно отделяются от всех остальных классов.
Поэтому, для достижения приемлемого качества классификации имеет смысл рассматривать эти классы отдельно и выбирать бинарный классификатор независимо от остальных классов.

\subsubsection{Модель авторегрессии~\ref{eq:autoregressive_model} и Фурье~\ref{eq:fourier}}

\paragraph{Признаковое описание.}
\label{par:ar_fourier_feature_selection_USCHAD}
Возьмем признаковое описание временных рядов из предыдущего эксперимента,~\fullref{par:ar_feature_selection_USCHAD}, и добавим к нему первые~$5$ коэффициентов Фурье~\ref{eq:fourier}.
Таким образом, каждый $6$"~компонентный временной ряд будет описываться $128$~признаками.

\paragraph{Классификатор.}
Задача многоклассовой классификации сводилась к задаче бинарной классификации при помощи подхода One-vs-One и квадратичной функцией потерь.
В качестве бинарного классификатора использовался SVM с гауссовским ядром и параметрами $C=16,\ \gamma=0.04$.

\paragraph{Результаты.}
На диаграмме ниже приведено качество классификации, усредненное по~$m=100$ случайным разбиениям исходной выборки на тестовую и контрольную в отношении $7$ к $3$.

\begin{minipage}{\textwidth}
\begin{minipage}{0.49\textwidth}
\begin{figure}[H]
\includegraphics[width=\textwidth]{{Accuracy_Dataset_USCHAD_fourier_AR_nSplits_100_rate_0.7_approach_OneVsOne_HD_classifier_SVM_-t2-c16-g0.04}.eps}
\caption{Точность классификации для параметров модели авторегрессии в качестве признаковых описаний.
Dataset:~USC-HAD.
Под Mean accuracy понимается значение функционала~\ref{eq:total_quality}.
Над столбцами приведены средние точности классификации для каждого класса по формуле~\ref{eq:class_quality}.}
\label{fig:USCHAD_AR_FOURIER}
\end{figure}
\end{minipage}
\quad
\begin{minipage}{0.49\textwidth}
\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
  \cline{3-14}
  \multicolumn{2}{c|}{} & \multicolumn{12}{c|}{Predicted class} \\ \cline{3-14}
  \multicolumn{2}{c|}{} & $1$  & $2$  & $3$  & $4$  & $5$  & $6$  & $7$  & $8$  & $9$ & $10$ & $11$ & $12$\\ \cline{1-14}
  \multirow{12}{*}{\begin{sideways}Actual class\end{sideways}}
  & $1$ & $\vec{0.97}$ & $0.01$ & $0.01$ & $0.01$ & $0.01$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$\\ \cline{2-14}
  & $2$ & $0.01$ & $\vec{0.98}$ & $0.01$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$\\ \cline{2-14}
  & $3$ & $0.00$ & $0.00$ & $\vec{1.00}$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$\\ \cline{2-14}
  & $4$ & $0.00$ & $0.00$ & $0.00$ & $\vec{0.99}$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$\\ \cline{2-14}
  & $5$ & $0.02$ & $0.00$ & $0.00$ & $0.01$ & $\vec{0.95}$ & $0.02$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$\\ \cline{2-14}
  & $6$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $\vec{1.00}$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$\\ \cline{2-14}
  & $7$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $\vec{0.99}$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$\\ \cline{2-14}
  & $8$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $\vec{0.91}$ & $0.08$ & $0.00$ & $0.00$ & $0.01$\\ \cline{2-14}
  & $9$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.05$ & $\vec{0.95}$ & $0.00$ & $0.00$ & $0.00$\\ \cline{2-14}
  & $10$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $\vec{1.00}$ & $0.00$ & $0.00$\\ \cline{2-14}
  & $11$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $\vec{1.00}$ & $0.00$\\ \cline{2-14}
  & $12$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.00$ & $0.01$ & $0.00$ & $0.00$ & $0.01$ & $\vec{0.98}$\\ \cline{1-14}
\end{tabular}}
\caption{Mean confusion matrix. Признаки, порожденные моделью авторегрессии. Dataset:~USC-HAD.}
\label{tbl:USCHAD_AR_FOURIER_confusion}
\end{table}
\end{minipage}
\end{minipage}

Из таблицы~\ref{tbl:USCHAD_AR_FOURIER_confusion} видно, что использование коэффициентов Фурье значительно повысило качество классификации.
Классы~$8$~и~$9$  (sit and fidget и stand) достоверно отделяются от всех остальных десяти классов.
Поэтому повысить точность для них можно, настраивая отдельно на этих двух классах бинарный классификатор, их разделяющий в подходе One-vs-One.


\subsubsection{Классификация голосованием и классификация в пространстве распределений параметров}
Рассмотрим, наконец, алгоритм классификации в сочетании с процедурой сегментации временных рядов.
В качестве процедуры сегментации~$S(x)$~(см.~\ref{eq:segmentation}) будем использовать случайное выделение сегментов переменной длины.
Для каждого временного ряда получаем $20$~сегментов.
Будем решать задачу классификации только для первых $10$~классов (за исключением <<elevator up>> и <<elevator down>>, которые плохо отделяются друг от друга при малой длине сегментов, двумя алгоритмами.

В алгоритме голосования классификатор~$a(x)$ обучается на новой обучающей выборке для сегментов исходных временных рядов
\[
\widehat{\mathfrak{D}}=\left\{(\vec{f}_g(s),y):\;(x,y)\in\mathfrak{D},\,s\in S(x)\right\}.
\]
А последующая классификация производится голосованием:
\[
\text{mode}\left\{a(\vec{f}_g(s)):\;s\in S(x)\right\}.
\]

Алгоритм классификации в пространстве гиперпараметров (распределений параметров аппроксимирующих моделей был описан в~\fullref{sec:distribution}.
В эксперименте использовалось семейство нормальных распределений с диагональной ковариационной матрицей.

Задача многоклассовой классификации решалась при помощи подхода One-vs-One бинарными классификаторами SVM с линейным ядром и параметром~$C=0.25$.

На графике~\ref{fig:USCHAD_AR_VOTING_VS_DISTR} ниже приведены результаты для средней точности решения задачи многоклассовой классификации обоими алгоритмами.
\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{{VotingSegmentsVsNormalDistribution_Dataset_USCHAD_nSplits_5_rate_0.7_approach_OneVsOne_HD_classifier_SVMLinear_-c0.25}.eps}
\caption{Зависимость средней точности классификации от длины случайных сегментов.
Dataset:~USC-HAD, учитываются только первые $10$~классов.
Под Mean accuracy понимается значение функционала~\ref{eq:total_quality}.}
\label{fig:USCHAD_AR_VOTING_VS_DISTR}
\end{figure}

Из графика можно видеть, что оба алгоритма позволяют повысить точность классификации на~$1\%$ при длине сегмента~$50$.
Однако, при любой длине сегмента алгоритм голосования показывает лучшие результаты.

% \subsection{Датасет MIT-BIH Arrhythmia Database~\cite{Goldberger13062000}}

\bigskip
\section{Обсуждение}
Сравнение результатов

\bigskip
\section{Заключение}
В работе было показано, что метод признакового описания временного ряда оптимальными параметрами его аппроксимирующих моделей дает высокое качество решения задачи классификации.
Предложенный метод вычислительно эффективен и не требователен к памяти вычислительного устройства.
В работе также был предложен алгоритм классификации временных рядов в пространстве распределений параметров порождающих их сегменты моделей, который обобщает предыдущий метод классификации временных рядов и позволяет производить более тонкую настройку алгоритма классификации.
Однако, вычислительный эксперимент на данных задачи классификации типов физической активности показал, что в сочетании с процедурой случайной сегментации алгоритм голосования сегментов позволяет добиться лучших результатов.

\bibliography{Karasikov2015TimeSeriesClassification}
\end{document}